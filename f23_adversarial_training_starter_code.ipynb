{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7COyn-xqCC0g"
      },
      "source": [
        "# Adversarial Training\n",
        "\n",
        "As we saw before, classifiers can be tricked by a adversary (i.e. the bad guy) using attacks such as FGSM. We would like our classifier to be robust against these kind of attacks because misclassifing a `stop sign` as a `speed limit 100 miles` would not be ideal.\n",
        "\n",
        "Here comes **Adversarial Training**, where we train our model with our original data *along with generated adversarial examples*. If our model sees more adversarial examples, it can handle them better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgGkZxDwUtem"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEjNBZQQCWN1"
      },
      "outputs": [],
      "source": [
        "# Pytorch - Machine Learning Library\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, utils, optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "\n",
        "import tensorflow as tf # the other machine learning library\n",
        "\n",
        "import requests # this lets us make http requests, so we can use this to download things from the internet\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# libraries to help us process the images\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krzFZ6FWZ64f"
      },
      "source": [
        "### Setting device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHOI5nFyEscP",
        "outputId": "637e97de-349b-445a-83b0-2c88178e1e40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f'Running on {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP5cImkf_sLT"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEX9a59QE2Y9"
      },
      "outputs": [],
      "source": [
        "train_dataset = torchvision.datasets.CIFAR10(root='./cifar10', transform=torchvision.transforms.ToTensor(), download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./cifar10', train=False, transform=torchvision.transforms.ToTensor(), download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_lcQjTYE4GO"
      },
      "outputs": [],
      "source": [
        "batch_size = 128;\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqIIVu0iEuZo"
      },
      "outputs": [],
      "source": [
        "# visualizing a sample from train loader\n",
        "\n",
        "train_iter = iter(train_loader)\n",
        "\n",
        "batch_images, batch_labels = next(train_iter)\n",
        "image, label = batch_images[0], batch_labels[0]\n",
        "plt.imshow(image.permute(1,2,0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3Icu5wo_1Wc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-XQmc5j_1pf"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYB1MDAx_171"
      },
      "source": [
        "# Building Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuZ3UvW2FKYJ"
      },
      "source": [
        "Remember, that CNN's have a typical architecture that involve CONV -> Maxpool -> .... -> FC -> ... Output\n",
        "\n",
        "https://pytorch.org/docs/stable/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQmhlPR4E5bt"
      },
      "outputs": [],
      "source": [
        "class SillyBoiModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(SillyBoiModel, self).__init__()\n",
        "        self.ConvLayer1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.ConvLayer2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.ConvLayer3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.ConvLayer4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.ConvLayer5 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
        "        self.batchnorm4 = nn.BatchNorm2d(128)\n",
        "        self.batchnorm5 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc1 = nn.Linear(4*4*128, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ConvLayer1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.batchnorm1(x)\n",
        "\n",
        "        x = self.ConvLayer2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.batchnorm2(x)\n",
        "\n",
        "        x = self.ConvLayer3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.batchnorm3(x)\n",
        "\n",
        "        x = self.ConvLayer4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.batchnorm4(x)\n",
        "        # print(x.size())\n",
        "\n",
        "        x = self.ConvLayer5(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.batchnorm5(x)\n",
        "        # print(x.size())\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QGZYj5yE7A0"
      },
      "outputs": [],
      "source": [
        "MikeTheModel = SillyBoiModel(3, 10).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(MikeTheModel.parameters(), lr=0.003)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYm4ORBv_55Z"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkoDsrPlE8Yn"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    # what is the first thing to do before starting training?\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for i, batch in enumerate(train_loader):  # looping through\n",
        "        inputs, labels = batch\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = MikeTheModel(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      print('End of epoch loss:', round(loss.item(), 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIoH_wvs_99m"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ-8l76AE90s"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader, device):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "  class_correct = list(0. for i in range(10))\n",
        "  class_total = list(0. for i in range(10))\n",
        "  # since not training, don't need to calculate gradients\n",
        "  with torch.no_grad():\n",
        "    # print(\"TEST BP 1\")\n",
        "    for batch in test_loader:\n",
        "      # print(\"TEST BP 1.5\")\n",
        "      inputs, labels = batch\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      # print(\"TEST BP 2\")\n",
        "      outputs = MikeTheModel(inputs)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      # print(\"TEST BP 3\")\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      c = (predicted == labels).squeeze()\n",
        "      for i in range(4):\n",
        "        label = labels[i]\n",
        "        class_correct[label] += c[i].item()\n",
        "        class_total[label] += 1\n",
        "      # print(\"TEST BP 4\")\n",
        "  print('Accuracy of the network on the 10000 val images: %d %%' % (100 * correct / total))\n",
        "\n",
        "  for i in range(10):\n",
        "    print('Accuracy of %5s: %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca3DXD-QAByf"
      },
      "source": [
        "# Running the train-test loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-RfdZaDE_sy"
      },
      "outputs": [],
      "source": [
        "# NUM_EPOCHS = 15\n",
        "\n",
        "# for epoch in range(NUM_EPOCHS):\n",
        "#     print(\"Epoch: \", epoch + 1)\n",
        "#     train_one_epoch(MikeTheModel, train_loader, optimizer, criterion, device)\n",
        "#     test(MikeTheModel, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtL0ZIFRAzxQ"
      },
      "source": [
        "## Saving the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZdq7e-2I7qr"
      },
      "outputs": [],
      "source": [
        "# TODO: save the weights of your model (5 min) hint: look at the documentation or slides :))\n",
        "\n",
        "torch.save(MikeTheModel.state_dict(), \"model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RgnJ-Z1A2zh"
      },
      "source": [
        "## Loading the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIvzBWIENNvR"
      },
      "outputs": [],
      "source": [
        "# power outage!\n",
        "# you've lost all your weights.\n",
        "# or have you?\n",
        "# TODO: reload the weights you just (hopefully) saved (5 min)\n",
        "\n",
        "\"\"\"\n",
        "MikealaTheModel = SillyBoiModel(3, 10)\n",
        "MikealaTheModel.load_state_dict(torch.load(\"model.pth\"))\n",
        "MikealaTheModel.to(device)\n",
        "MikealaTheModel.eval()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrxdweQwtUd2"
      },
      "source": [
        "# Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4nwvSKmCwJ8"
      },
      "outputs": [],
      "source": [
        "MichelleTheModel = SillyBoiModel(3, 10).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fx07E6PHFxW"
      },
      "outputs": [],
      "source": [
        "# with benson's info\n",
        "\n",
        "\"\"\"\n",
        "Make FGSMTransorm Class\n",
        "Use this transform class to\n",
        "\"\"\"\n",
        "\n",
        "class FGSMTransform:\n",
        "  def __init__(self, epsilon):\n",
        "    self.epsilon = epsilon\n",
        "    self.network = SillyBoiModel(3, 10).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")) # I THINK?????\n",
        "    # self.model.load_state_dict(torch.load(\"./models/cnn.pth\"))\n",
        "    self.criterion = torch.nn.CrossEntropyLoss()\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  def __call__(self, sample):  # unsure about the squeezing\n",
        "    image, label = sample\n",
        "    image = image.to(self.device)\n",
        "    label = label.to(self.device)\n",
        "\n",
        "    image.requires_grad = True\n",
        "\n",
        "    output = self.network(image.unsqueeze(dim=0))[0]\n",
        "    prediction = output.max(dim=0)[1].cpu().numpy()\n",
        "    loss = F.nll_loss(output, label)\n",
        "    # loss.requires_grad = True\n",
        "\n",
        "    image_gradients = torch.autograd.grad(loss, image)[0]\n",
        "    image_gradients = image_gradients.sign()\n",
        "\n",
        "    # fast gradient sign attack\n",
        "    image_attacked = (image + self.epsilon * image_gradients).clamp(0, 1)  # add noise to image\n",
        "\n",
        "\n",
        "    return image_attacked.squeeze() # may not need to squeeze, idk\n",
        "\n",
        "class CustomCIFAR(datasets.CIFAR10): # straight from Benson's code\n",
        "    def __getitem__(self, index: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform is not None:\n",
        "            #CHANGED from the torchvision implementation: pass the target into transform\n",
        "            img = self.transform((img, target))\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "\n",
        "        return img, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vv5vRyUBY73g"
      },
      "outputs": [],
      "source": [
        "# lifted straight from Beson's code\n",
        "class ToTensor:\n",
        "    \"\"\"Convert ndarrays in sample to Tensors. Works the same as transforms.ToTensor() but includes labels\"\"\"\n",
        "    def __call__(self, sample):\n",
        "        x, label = sample\n",
        "        # requires_grad = True\n",
        "        return (transforms.functional.to_tensor(x), torch.from_numpy(np.array(label)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYZLCVZZoFU8"
      },
      "outputs": [],
      "source": [
        "class CustomCIFAR2(datasets.CIFAR10):\n",
        "  def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        train: bool = True,\n",
        "        transform  = None,\n",
        "        target_transform = None,\n",
        "        download: bool = False,\n",
        "        processing = None,\n",
        "    ) -> None:\n",
        "    super().__init__(root, train, transform, target_transform, download)\n",
        "    if processing:\n",
        "      # print(self.data[0], type(self.data), \"shape:\", self.data[0].shape)\n",
        "      processed_data = []\n",
        "\n",
        "      to_tensor_f = ToTensor()\n",
        "      for datapt, target in zip(self.data, self.targets):\n",
        "          processed_datapt = processing(to_tensor_f((datapt, target)))\n",
        "          # we need to transpose processed_data\n",
        "          # But first we need to move it to the cpu and turn it back to a numpy array\n",
        "          processed_datapt = processed_datapt.detach().cpu().numpy()\n",
        "\n",
        "          # processed_datapt = np.transpose(processed_datapt, axes = (2,1,0))\n",
        "          # ??????? Need to denormalize\n",
        "          # processed_datapt = (processed_datapt * 255).astype(np.uint8)\n",
        "          #  ???????????\n",
        "          processed_data.append(processed_datapt)\n",
        "\n",
        "      self.data = np.array(processed_data)\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          index (int): Index\n",
        "      Returns:\n",
        "          tuple: (image, target) where target is index of the target class.\n",
        "      \"\"\"\n",
        "      img, target = self.data[index], self.targets[index]\n",
        "      return img, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV5EpWELtiWH"
      },
      "outputs": [],
      "source": [
        "# DUMP OF STUFF\n",
        "\n",
        "\"\"\"\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./cifar10', transform=torchvision.transforms.ToTensor(), download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./cifar10', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "\n",
        "batch_size = 128;\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\"\"\"\n",
        "\n",
        "MikeTheModel.train()\n",
        "\n",
        "EPSILONS = [0.005, 0.01, 0.015, 0.2]\n",
        "\n",
        "for epsilon in EPSILONS: # make EPSILONS -- ALSO A WIP\n",
        "  print(\"TRAINGING WITH EPSILON: \", epsilon)\n",
        "\n",
        "  fgsm_transform = transforms.Compose([ToTensor(),FGSMTransform(epsilon)])\n",
        "\n",
        "  train_dataset2 = CustomCIFAR(root='./cifar10', train=True, transform=fgsm_transform, download=True)\n",
        "  perturbed_train_dataset = torch.utils.data.ConcatDataset([train_dataset, train_dataset2])\n",
        "  perturbed_train_loader = torch.utils.data.DataLoader(dataset=train_dataset2, batch_size=batch_size, shuffle=True)\n",
        "  test_dataset2 = CustomCIFAR2(root='./cifar10', train=False, processing=FGSMTransform(epsilon), download=True)\n",
        "  perturbed_test_dataset = torch.utils.data.ConcatDataset([test_dataset, test_dataset2])\n",
        "  perturbed_test_loader = torch.utils.data.DataLoader(dataset=test_dataset2, batch_size=batch_size, shuffle=True)\n",
        "  PERTURBED_NUM_EPOCHS = 5\n",
        "\n",
        "  for epoch in range(PERTURBED_NUM_EPOCHS):\n",
        "    print(\"Epoch: \", epoch + 1)\n",
        "    train_one_epoch(MikeTheModel, perturbed_train_loader, optimizer, criterion, device)\n",
        "    test(MikeTheModel, perturbed_test_loader, device) # need to make a perturbed_test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDQLpjql5fIo"
      },
      "source": [
        "### DEBUG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7dnWRMGorWc"
      },
      "outputs": [],
      "source": [
        "# test(MikeTheModel, perturbed_test_loader, device)\n",
        "# train_one_epoch(MikeTheModel, perturbed_train_loader, optimizer, criterion, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RB1pcyagu3KM"
      },
      "outputs": [],
      "source": [
        "print(test_dataset2)\n",
        "print(len(test_dataset2))\n",
        "perturbed_test_loader = torch.utils.data.DataLoader(dataset=test_dataset2, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4g4bOdToCAz"
      },
      "outputs": [],
      "source": [
        "for data in perturbed_test_dataset:\n",
        "  print(data)\n",
        "  break\n",
        "for data in perturbed_train_dataset:\n",
        "  print(data)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU9sjm1lXxFv"
      },
      "outputs": [],
      "source": [
        "for param in MikeTheModel.parameters():\n",
        "    print(param.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqxkzmDp3-W-"
      },
      "outputs": [],
      "source": [
        "for data in test_dataset:\n",
        "  print(data)\n",
        "  break\n",
        "for data in train_dataset:\n",
        "  print(data)\n",
        "  break\n",
        "test_dataset2 = CustomCIFAR(root='./cifar10', train=False, transform=fgsm_transform, download=True)\n",
        "for data in test_dataset2:\n",
        "  print(data[0].requires_grad)\n",
        "  print(data[0].grad_fn)\n",
        "  break\n",
        "for data in perturbed_test_loader:\n",
        "  print(data)\n",
        "  break\n",
        "with torch.no_grad():\n",
        "  for data in perturbed_test_loader:\n",
        "    print(data)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC8P7Ig_j-F6"
      },
      "outputs": [],
      "source": [
        "print(type(test_dataset2))\n",
        "print(type(test_dataset))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "AtL0ZIFRAzxQ",
        "8RgnJ-Z1A2zh"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
